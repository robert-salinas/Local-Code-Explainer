# ADR-0001: Uso de Ollama para LLMs Locales

## Estado
Aceptado

## Contexto
Necesitamos una forma de ejecutar modelos de lenguaje potentes (Mistral, Llama) localmente en la m치quina del usuario para garantizar la privacidad y eliminar la dependencia de APIs externas costosas.

## Decisi칩n
Utilizaremos **Ollama** como el motor de inferencia local. Ollama proporciona una API sencilla, gesti칩n eficiente de modelos y soporte para una amplia variedad de arquitecturas de LLM.

## Consecuencias
- **Positivas**: Facilidad de instalaci칩n para el usuario, alto rendimiento en hardware local, API compatible con JSON.
- **Negativas**: El usuario debe tener Ollama instalado y espacio en disco para los modelos (~4GB+).
